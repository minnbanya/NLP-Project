{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.2+cu121', '0.7.1', '0.17.2+cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchdata.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234 #change three times\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>questionText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toys_and_Games</td>\n",
       "      <td>Many have stated similar to the following: \"Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Health_and_Personal_Care</td>\n",
       "      <td>Will these work with the Phillips sonicare han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cell_Phones_and_Accessories</td>\n",
       "      <td>What kind of sim card it use?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen</td>\n",
       "      <td>does anyone know if this dinnerware set does n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Musical_Instruments</td>\n",
       "      <td>I'm thinking of getting in to modular synthesi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      category  \\\n",
       "0               Toys_and_Games   \n",
       "1     Health_and_Personal_Care   \n",
       "2  Cell_Phones_and_Accessories   \n",
       "3             Home_and_Kitchen   \n",
       "4          Musical_Instruments   \n",
       "\n",
       "                                        questionText  \n",
       "0  Many have stated similar to the following: \"Pa...  \n",
       "1  Will these work with the Phillips sonicare han...  \n",
       "2                      What kind of sim card it use?  \n",
       "3  does anyone know if this dinnerware set does n...  \n",
       "4  I'm thinking of getting in to modular synthesi...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data/train-qar.jsonl', lines=True, nrows=300000)\n",
    "df = df[['category', 'questionText']]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Toys_and_Games', 'Health_and_Personal_Care',\n",
       "       'Cell_Phones_and_Accessories', 'Home_and_Kitchen',\n",
       "       'Musical_Instruments', 'Baby', 'Sports_and_Outdoors',\n",
       "       'Patio_Lawn_and_Garden', 'Video_Games', 'Pet_Supplies',\n",
       "       'Tools_and_Home_Improvement', 'Beauty', 'Electronics',\n",
       "       'Grocery_and_Gourmet_Food', 'Automotive', 'Office_Products',\n",
       "       'Clothing_Shoes_and_Jewelry'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Product category\n",
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text to numeric class\n",
    "class_mapping = {\n",
    "    'Toys_and_Games': 0,\n",
    "    'Health_and_Personal_Care': 1,\n",
    "    'Cell_Phones_and_Accessories': 2,\n",
    "    'Home_and_Kitchen': 3,\n",
    "    'Musical_Instruments': 4,\n",
    "    'Baby': 5,\n",
    "    'Sports_and_Outdoors': 6,\n",
    "    'Patio_Lawn_and_Garden': 7,\n",
    "    'Video_Games': 8,\n",
    "    'Pet_Supplies': 9,\n",
    "    'Tools_and_Home_Improvement': 10,\n",
    "    'Beauty': 11,\n",
    "    'Electronics': 12,\n",
    "    'Grocery_and_Gourmet_Food': 13,\n",
    "    'Automotive': 14,\n",
    "    'Office_Products': 15,\n",
    "    'Clothing_Shoes_and_Jewelry': 16\n",
    "}\n",
    "\n",
    "# Map class names to numerical labels\n",
    "df['category'] = df['category'].map(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 100 data in each classes\n",
    "df_sample = df.groupby('category', group_keys=False).apply(lambda x: x.sample(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "df_sample['questionText']  =  df_sample['questionText'].apply(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data):\n",
    "    regex_s = re.sub(\"\\\\(.+?\\\\)|[\\r\\n|\\n\\r]|!\", \"\", data)\n",
    "    fin = \" \".join(regex_s.split())\n",
    "    return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['questionText'] = df_sample['questionText'].apply(data_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_sample, test_size=0.1,stratify=df_sample['category'], random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df, test_size=0.1, stratify=train_df['category'],random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "7     810\n",
       "13    810\n",
       "14    810\n",
       "9     810\n",
       "10    810\n",
       "8     810\n",
       "6     810\n",
       "15    810\n",
       "16    810\n",
       "1     810\n",
       "0     810\n",
       "11    810\n",
       "5     810\n",
       "12    810\n",
       "3     810\n",
       "2     810\n",
       "4     810\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'is', 'the', 'best', 'product', '?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens    = tokenizer(\"What is the best product?\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to integers (numeral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data):\n",
    "    for data_sample in data:\n",
    "        yield tokenizer(data_sample) \n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_df['questionText']), specials = ['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[376, 8, 7]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'it', 'is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = vocab.get_itos()\n",
    "mapping[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13987"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13987, 300])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrap(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataframe.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category                                                        7\n",
       "questionText    can i use this to divert water into a soaker t...\n",
       "Name: 144730, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataWrap(train_df)\n",
    "valid = DataWrap(val_df)\n",
    "test = DataWrap(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x)) #{hello world this is yt} => {'hello', 'world', 'this', 'is', 'yt'} => {4, 88, 11, 22, 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 71, 981, 13104, 0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(\"I am currently teaching LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "pad_idx = vocab['<pad>'] \n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    #criterion expects float labels\n",
    "    return torch.tensor(label_list, dtype=torch.int64), pad_sequence(text_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True,  collate_fn=collate_batch) #num_workers to train faster\n",
    "val_loader   = DataLoader(valid, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text in val_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape #(batch_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 33])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape #(batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): #more elegant version\n",
    "    def __init__(self, input_dim, emb_dim, output_dim, dropout, n_filters, filter_sizes):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, emb_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, (nn.Conv2d, nn.Conv2d)):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.kaiming_normal_(param) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    \n",
    "    for i, (label, text) in enumerate(loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "                        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions = model(text).squeeze(1) \n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(val_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/pydantic/_internal/_config.py:334: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/892042135543675506', creation_time=1713636573294, experiment_id='892042135543675506', last_update_time=1713636573294, lifecycle_stage='active', name='CNN1000', tags={}>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#experiment tracking\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import os\n",
    "\n",
    "# This the dockerized method.\n",
    "# We build two docker containers, one for python/jupyter and another for mlflow.\n",
    "# The url `mlflow` is resolved into another container within the same composer.\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "# In the dockerized way, the user who runs this code will be `root`.\n",
    "# The MLflow will also log the run user_id as `root`.\n",
    "# To change that, we need to set this environ[\"LOGNAME\"] to your name.\n",
    "os.environ[\"LOGNAME\"] = \"noppawee\"\n",
    "#mlflow.create_experiment(name=\"noppawee-ML-project\")  #create if you haven't create\n",
    "mlflow.set_experiment(experiment_name=\"CNN1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== CNN with 10-epochs-50-n_filters =====\n",
      "Epoch: 1 | Time: 0m 15s\n",
      "\tTrain Loss: 2.832 | Train Acc: 10.61%\n",
      "\tVal.  Loss: 2.640 | Val Acc: 20.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Time: 0m 16s\n",
      "\tTrain Loss: 2.572 | Train Acc: 20.87%\n",
      "\tVal.  Loss: 2.444 | Val Acc: 26.40%\n",
      "Epoch: 3 | Time: 0m 16s\n",
      "\tTrain Loss: 2.397 | Train Acc: 26.17%\n",
      "\tVal.  Loss: 2.318 | Val Acc: 28.62%\n",
      "Epoch: 4 | Time: 0m 17s\n",
      "\tTrain Loss: 2.284 | Train Acc: 30.27%\n",
      "\tVal.  Loss: 2.243 | Val Acc: 30.93%\n",
      "Epoch: 5 | Time: 0m 17s\n",
      "\tTrain Loss: 2.191 | Train Acc: 32.75%\n",
      "\tVal.  Loss: 2.187 | Val Acc: 32.14%\n",
      "Epoch: 6 | Time: 0m 17s\n",
      "\tTrain Loss: 2.127 | Train Acc: 34.77%\n",
      "\tVal.  Loss: 2.147 | Val Acc: 34.02%\n",
      "Epoch: 7 | Time: 0m 16s\n",
      "\tTrain Loss: 2.064 | Train Acc: 36.79%\n",
      "\tVal.  Loss: 2.116 | Val Acc: 34.56%\n",
      "Epoch: 8 | Time: 0m 15s\n",
      "\tTrain Loss: 2.015 | Train Acc: 38.46%\n",
      "\tVal.  Loss: 2.081 | Val Acc: 35.22%\n",
      "Epoch: 9 | Time: 0m 15s\n",
      "\tTrain Loss: 1.957 | Train Acc: 40.83%\n",
      "\tVal.  Loss: 2.056 | Val Acc: 36.00%\n",
      "Epoch: 10 | Time: 0m 17s\n",
      "\tTrain Loss: 1.912 | Train Acc: 42.32%\n",
      "\tVal.  Loss: 2.042 | Val Acc: 37.65%\n",
      "===== CNN with 10-epochs-100-n_filters =====\n",
      "Epoch: 1 | Time: 0m 22s\n",
      "\tTrain Loss: 2.823 | Train Acc: 10.81%\n",
      "\tVal.  Loss: 2.592 | Val Acc: 22.56%\n",
      "Epoch: 2 | Time: 0m 21s\n",
      "\tTrain Loss: 2.512 | Train Acc: 22.04%\n",
      "\tVal.  Loss: 2.406 | Val Acc: 26.90%\n",
      "Epoch: 3 | Time: 0m 21s\n",
      "\tTrain Loss: 2.334 | Train Acc: 28.32%\n",
      "\tVal.  Loss: 2.293 | Val Acc: 28.79%\n",
      "Epoch: 4 | Time: 0m 21s\n",
      "\tTrain Loss: 2.215 | Train Acc: 32.35%\n",
      "\tVal.  Loss: 2.221 | Val Acc: 30.95%\n",
      "Epoch: 5 | Time: 0m 24s\n",
      "\tTrain Loss: 2.114 | Train Acc: 35.54%\n",
      "\tVal.  Loss: 2.164 | Val Acc: 32.72%\n",
      "Epoch: 6 | Time: 0m 22s\n",
      "\tTrain Loss: 2.035 | Train Acc: 38.51%\n",
      "\tVal.  Loss: 2.127 | Val Acc: 33.40%\n",
      "Epoch: 7 | Time: 0m 22s\n",
      "\tTrain Loss: 1.976 | Train Acc: 40.36%\n",
      "\tVal.  Loss: 2.088 | Val Acc: 35.93%\n",
      "Epoch: 8 | Time: 0m 21s\n",
      "\tTrain Loss: 1.908 | Train Acc: 42.41%\n",
      "\tVal.  Loss: 2.069 | Val Acc: 35.63%\n",
      "Epoch: 9 | Time: 0m 21s\n",
      "\tTrain Loss: 1.847 | Train Acc: 43.89%\n",
      "\tVal.  Loss: 2.044 | Val Acc: 36.61%\n",
      "Epoch: 10 | Time: 0m 21s\n",
      "\tTrain Loss: 1.784 | Train Acc: 46.58%\n",
      "\tVal.  Loss: 2.028 | Val Acc: 37.06%\n",
      "===== CNN with 10-epochs-150-n_filters =====\n",
      "Epoch: 1 | Time: 0m 24s\n",
      "\tTrain Loss: 2.819 | Train Acc: 11.84%\n",
      "\tVal.  Loss: 2.557 | Val Acc: 22.96%\n",
      "Epoch: 2 | Time: 0m 25s\n",
      "\tTrain Loss: 2.498 | Train Acc: 22.85%\n",
      "\tVal.  Loss: 2.372 | Val Acc: 27.48%\n",
      "Epoch: 3 | Time: 0m 24s\n",
      "\tTrain Loss: 2.306 | Train Acc: 29.55%\n",
      "\tVal.  Loss: 2.245 | Val Acc: 32.79%\n",
      "Epoch: 4 | Time: 0m 26s\n",
      "\tTrain Loss: 2.174 | Train Acc: 33.66%\n",
      "\tVal.  Loss: 2.182 | Val Acc: 32.46%\n",
      "Epoch: 5 | Time: 0m 24s\n",
      "\tTrain Loss: 2.083 | Train Acc: 36.52%\n",
      "\tVal.  Loss: 2.133 | Val Acc: 34.91%\n",
      "Epoch: 6 | Time: 0m 25s\n",
      "\tTrain Loss: 1.996 | Train Acc: 40.01%\n",
      "\tVal.  Loss: 2.092 | Val Acc: 35.74%\n",
      "Epoch: 7 | Time: 0m 24s\n",
      "\tTrain Loss: 1.922 | Train Acc: 41.87%\n",
      "\tVal.  Loss: 2.065 | Val Acc: 36.86%\n",
      "Epoch: 8 | Time: 0m 24s\n",
      "\tTrain Loss: 1.859 | Train Acc: 43.77%\n",
      "\tVal.  Loss: 2.039 | Val Acc: 36.95%\n",
      "Epoch: 9 | Time: 0m 25s\n",
      "\tTrain Loss: 1.796 | Train Acc: 46.26%\n",
      "\tVal.  Loss: 2.014 | Val Acc: 37.76%\n",
      "Epoch: 10 | Time: 0m 26s\n",
      "\tTrain Loss: 1.728 | Train Acc: 48.21%\n",
      "\tVal.  Loss: 2.007 | Val Acc: 37.89%\n",
      "===== CNN with 20-epochs-50-n_filters =====\n",
      "Epoch: 1 | Time: 0m 15s\n",
      "\tTrain Loss: 2.805 | Train Acc: 11.19%\n",
      "\tVal.  Loss: 2.627 | Val Acc: 21.81%\n",
      "Epoch: 2 | Time: 0m 15s\n",
      "\tTrain Loss: 2.534 | Train Acc: 21.99%\n",
      "\tVal.  Loss: 2.409 | Val Acc: 28.09%\n",
      "Epoch: 3 | Time: 0m 16s\n",
      "\tTrain Loss: 2.355 | Train Acc: 27.45%\n",
      "\tVal.  Loss: 2.282 | Val Acc: 30.18%\n",
      "Epoch: 4 | Time: 0m 15s\n",
      "\tTrain Loss: 2.238 | Train Acc: 31.33%\n",
      "\tVal.  Loss: 2.207 | Val Acc: 32.55%\n",
      "Epoch: 5 | Time: 0m 16s\n",
      "\tTrain Loss: 2.157 | Train Acc: 33.65%\n",
      "\tVal.  Loss: 2.158 | Val Acc: 33.51%\n",
      "Epoch: 6 | Time: 0m 15s\n",
      "\tTrain Loss: 2.083 | Train Acc: 36.42%\n",
      "\tVal.  Loss: 2.119 | Val Acc: 34.88%\n",
      "Epoch: 7 | Time: 0m 15s\n",
      "\tTrain Loss: 2.033 | Train Acc: 38.01%\n",
      "\tVal.  Loss: 2.097 | Val Acc: 36.29%\n",
      "Epoch: 8 | Time: 0m 15s\n",
      "\tTrain Loss: 1.973 | Train Acc: 39.41%\n",
      "\tVal.  Loss: 2.062 | Val Acc: 36.86%\n",
      "Epoch: 9 | Time: 0m 16s\n",
      "\tTrain Loss: 1.924 | Train Acc: 41.00%\n",
      "\tVal.  Loss: 2.051 | Val Acc: 37.06%\n",
      "Epoch: 10 | Time: 0m 15s\n",
      "\tTrain Loss: 1.874 | Train Acc: 43.38%\n",
      "\tVal.  Loss: 2.032 | Val Acc: 37.15%\n",
      "Epoch: 11 | Time: 0m 16s\n",
      "\tTrain Loss: 1.834 | Train Acc: 44.18%\n",
      "\tVal.  Loss: 2.013 | Val Acc: 38.66%\n",
      "Epoch: 12 | Time: 0m 15s\n",
      "\tTrain Loss: 1.794 | Train Acc: 45.57%\n",
      "\tVal.  Loss: 2.010 | Val Acc: 38.10%\n",
      "Epoch: 13 | Time: 0m 16s\n",
      "\tTrain Loss: 1.747 | Train Acc: 47.33%\n",
      "\tVal.  Loss: 1.982 | Val Acc: 39.24%\n",
      "Epoch: 14 | Time: 0m 15s\n",
      "\tTrain Loss: 1.706 | Train Acc: 48.59%\n",
      "\tVal.  Loss: 1.976 | Val Acc: 39.56%\n",
      "Epoch: 15 | Time: 0m 17s\n",
      "\tTrain Loss: 1.670 | Train Acc: 49.90%\n",
      "\tVal.  Loss: 1.963 | Val Acc: 39.72%\n",
      "Epoch: 16 | Time: 0m 16s\n",
      "\tTrain Loss: 1.633 | Train Acc: 50.33%\n",
      "\tVal.  Loss: 1.960 | Val Acc: 39.98%\n",
      "Epoch: 17 | Time: 0m 16s\n",
      "\tTrain Loss: 1.589 | Train Acc: 52.45%\n",
      "\tVal.  Loss: 1.947 | Val Acc: 40.44%\n",
      "Epoch: 18 | Time: 0m 16s\n",
      "\tTrain Loss: 1.558 | Train Acc: 52.85%\n",
      "\tVal.  Loss: 1.942 | Val Acc: 39.89%\n",
      "Epoch: 19 | Time: 0m 15s\n",
      "\tTrain Loss: 1.524 | Train Acc: 54.27%\n",
      "\tVal.  Loss: 1.935 | Val Acc: 40.91%\n",
      "Epoch: 20 | Time: 0m 16s\n",
      "\tTrain Loss: 1.492 | Train Acc: 54.80%\n",
      "\tVal.  Loss: 1.927 | Val Acc: 40.64%\n",
      "===== CNN with 20-epochs-100-n_filters =====\n",
      "Epoch: 1 | Time: 0m 21s\n",
      "\tTrain Loss: 2.818 | Train Acc: 11.70%\n",
      "\tVal.  Loss: 2.569 | Val Acc: 22.26%\n",
      "Epoch: 2 | Time: 0m 22s\n",
      "\tTrain Loss: 2.475 | Train Acc: 24.01%\n",
      "\tVal.  Loss: 2.355 | Val Acc: 29.69%\n",
      "Epoch: 3 | Time: 0m 23s\n",
      "\tTrain Loss: 2.279 | Train Acc: 30.37%\n",
      "\tVal.  Loss: 2.227 | Val Acc: 32.59%\n",
      "Epoch: 4 | Time: 0m 21s\n",
      "\tTrain Loss: 2.152 | Train Acc: 34.48%\n",
      "\tVal.  Loss: 2.155 | Val Acc: 34.38%\n",
      "Epoch: 5 | Time: 0m 21s\n",
      "\tTrain Loss: 2.055 | Train Acc: 37.59%\n",
      "\tVal.  Loss: 2.102 | Val Acc: 35.78%\n",
      "Epoch: 6 | Time: 0m 22s\n",
      "\tTrain Loss: 1.978 | Train Acc: 40.14%\n",
      "\tVal.  Loss: 2.065 | Val Acc: 36.79%\n",
      "Epoch: 7 | Time: 0m 22s\n",
      "\tTrain Loss: 1.910 | Train Acc: 42.55%\n",
      "\tVal.  Loss: 2.037 | Val Acc: 38.09%\n",
      "Epoch: 8 | Time: 0m 21s\n",
      "\tTrain Loss: 1.847 | Train Acc: 44.23%\n",
      "\tVal.  Loss: 2.018 | Val Acc: 37.93%\n",
      "Epoch: 9 | Time: 0m 22s\n",
      "\tTrain Loss: 1.781 | Train Acc: 46.69%\n",
      "\tVal.  Loss: 1.996 | Val Acc: 38.12%\n",
      "Epoch: 10 | Time: 0m 22s\n",
      "\tTrain Loss: 1.731 | Train Acc: 47.82%\n",
      "\tVal.  Loss: 1.977 | Val Acc: 39.09%\n",
      "Epoch: 11 | Time: 0m 22s\n",
      "\tTrain Loss: 1.671 | Train Acc: 49.94%\n",
      "\tVal.  Loss: 1.959 | Val Acc: 39.67%\n",
      "Epoch: 12 | Time: 0m 22s\n",
      "\tTrain Loss: 1.624 | Train Acc: 51.42%\n",
      "\tVal.  Loss: 1.965 | Val Acc: 38.81%\n",
      "Epoch: 13 | Time: 0m 22s\n",
      "\tTrain Loss: 1.580 | Train Acc: 52.56%\n",
      "\tVal.  Loss: 1.946 | Val Acc: 40.16%\n",
      "Epoch: 14 | Time: 0m 22s\n",
      "\tTrain Loss: 1.532 | Train Acc: 54.01%\n",
      "\tVal.  Loss: 1.937 | Val Acc: 39.99%\n",
      "Epoch: 15 | Time: 0m 22s\n",
      "\tTrain Loss: 1.483 | Train Acc: 55.96%\n",
      "\tVal.  Loss: 1.927 | Val Acc: 40.99%\n",
      "Epoch: 16 | Time: 0m 21s\n",
      "\tTrain Loss: 1.430 | Train Acc: 57.13%\n",
      "\tVal.  Loss: 1.912 | Val Acc: 41.06%\n",
      "Epoch: 17 | Time: 0m 22s\n",
      "\tTrain Loss: 1.381 | Train Acc: 58.79%\n",
      "\tVal.  Loss: 1.902 | Val Acc: 41.32%\n",
      "Epoch: 18 | Time: 0m 21s\n",
      "\tTrain Loss: 1.338 | Train Acc: 60.31%\n",
      "\tVal.  Loss: 1.906 | Val Acc: 41.22%\n",
      "Epoch: 19 | Time: 0m 21s\n",
      "\tTrain Loss: 1.292 | Train Acc: 61.77%\n",
      "\tVal.  Loss: 1.897 | Val Acc: 41.89%\n",
      "Epoch: 20 | Time: 0m 21s\n",
      "\tTrain Loss: 1.248 | Train Acc: 63.07%\n",
      "\tVal.  Loss: 1.910 | Val Acc: 41.64%\n",
      "===== CNN with 20-epochs-150-n_filters =====\n",
      "Epoch: 1 | Time: 0m 25s\n",
      "\tTrain Loss: 2.808 | Train Acc: 12.11%\n",
      "\tVal.  Loss: 2.550 | Val Acc: 20.97%\n",
      "Epoch: 2 | Time: 0m 25s\n",
      "\tTrain Loss: 2.446 | Train Acc: 24.58%\n",
      "\tVal.  Loss: 2.328 | Val Acc: 29.12%\n",
      "Epoch: 3 | Time: 0m 26s\n",
      "\tTrain Loss: 2.243 | Train Acc: 31.51%\n",
      "\tVal.  Loss: 2.206 | Val Acc: 32.05%\n",
      "Epoch: 4 | Time: 0m 26s\n",
      "\tTrain Loss: 2.102 | Train Acc: 35.85%\n",
      "\tVal.  Loss: 2.154 | Val Acc: 32.83%\n",
      "Epoch: 5 | Time: 0m 25s\n",
      "\tTrain Loss: 2.006 | Train Acc: 39.05%\n",
      "\tVal.  Loss: 2.086 | Val Acc: 35.62%\n",
      "Epoch: 6 | Time: 0m 25s\n",
      "\tTrain Loss: 1.928 | Train Acc: 41.42%\n",
      "\tVal.  Loss: 2.051 | Val Acc: 36.36%\n",
      "Epoch: 7 | Time: 0m 25s\n",
      "\tTrain Loss: 1.843 | Train Acc: 44.45%\n",
      "\tVal.  Loss: 2.017 | Val Acc: 36.79%\n",
      "Epoch: 8 | Time: 0m 25s\n",
      "\tTrain Loss: 1.780 | Train Acc: 46.40%\n",
      "\tVal.  Loss: 1.995 | Val Acc: 37.70%\n",
      "Epoch: 9 | Time: 0m 24s\n",
      "\tTrain Loss: 1.713 | Train Acc: 48.30%\n",
      "\tVal.  Loss: 1.969 | Val Acc: 38.79%\n",
      "Epoch: 10 | Time: 0m 24s\n",
      "\tTrain Loss: 1.653 | Train Acc: 50.41%\n",
      "\tVal.  Loss: 1.963 | Val Acc: 38.34%\n",
      "Epoch: 11 | Time: 0m 24s\n",
      "\tTrain Loss: 1.587 | Train Acc: 52.71%\n",
      "\tVal.  Loss: 1.950 | Val Acc: 39.03%\n",
      "Epoch: 12 | Time: 0m 24s\n",
      "\tTrain Loss: 1.535 | Train Acc: 54.05%\n",
      "\tVal.  Loss: 1.956 | Val Acc: 38.93%\n",
      "Epoch: 13 | Time: 0m 25s\n",
      "\tTrain Loss: 1.478 | Train Acc: 55.83%\n",
      "\tVal.  Loss: 1.924 | Val Acc: 40.12%\n",
      "Epoch: 14 | Time: 0m 26s\n",
      "\tTrain Loss: 1.432 | Train Acc: 57.62%\n",
      "\tVal.  Loss: 1.921 | Val Acc: 39.93%\n",
      "Epoch: 15 | Time: 0m 24s\n",
      "\tTrain Loss: 1.379 | Train Acc: 59.36%\n",
      "\tVal.  Loss: 1.914 | Val Acc: 41.04%\n",
      "Epoch: 16 | Time: 0m 24s\n",
      "\tTrain Loss: 1.325 | Train Acc: 61.08%\n",
      "\tVal.  Loss: 1.918 | Val Acc: 40.88%\n",
      "Epoch: 17 | Time: 0m 24s\n",
      "\tTrain Loss: 1.275 | Train Acc: 62.40%\n",
      "\tVal.  Loss: 1.904 | Val Acc: 40.88%\n",
      "Epoch: 18 | Time: 0m 24s\n",
      "\tTrain Loss: 1.226 | Train Acc: 64.80%\n",
      "\tVal.  Loss: 1.889 | Val Acc: 41.17%\n",
      "Epoch: 19 | Time: 0m 26s\n",
      "\tTrain Loss: 1.188 | Train Acc: 65.33%\n",
      "\tVal.  Loss: 1.886 | Val Acc: 41.06%\n",
      "Epoch: 20 | Time: 0m 24s\n",
      "\tTrain Loss: 1.136 | Train Acc: 67.38%\n",
      "\tVal.  Loss: 1.900 | Val Acc: 40.75%\n",
      "===== CNN with 30-epochs-50-n_filters =====\n",
      "Epoch: 1 | Time: 0m 16s\n",
      "\tTrain Loss: 2.805 | Train Acc: 11.91%\n",
      "\tVal.  Loss: 2.595 | Val Acc: 24.10%\n",
      "Epoch: 2 | Time: 0m 15s\n",
      "\tTrain Loss: 2.498 | Train Acc: 24.16%\n",
      "\tVal.  Loss: 2.371 | Val Acc: 29.73%\n",
      "Epoch: 3 | Time: 0m 16s\n",
      "\tTrain Loss: 2.305 | Train Acc: 29.39%\n",
      "\tVal.  Loss: 2.238 | Val Acc: 32.59%\n",
      "Epoch: 4 | Time: 0m 15s\n",
      "\tTrain Loss: 2.173 | Train Acc: 33.58%\n",
      "\tVal.  Loss: 2.155 | Val Acc: 34.39%\n",
      "Epoch: 5 | Time: 0m 16s\n",
      "\tTrain Loss: 2.077 | Train Acc: 36.39%\n",
      "\tVal.  Loss: 2.098 | Val Acc: 37.08%\n",
      "Epoch: 6 | Time: 0m 16s\n",
      "\tTrain Loss: 2.003 | Train Acc: 39.18%\n",
      "\tVal.  Loss: 2.063 | Val Acc: 36.48%\n",
      "Epoch: 7 | Time: 0m 15s\n",
      "\tTrain Loss: 1.937 | Train Acc: 41.15%\n",
      "\tVal.  Loss: 2.028 | Val Acc: 37.58%\n",
      "Epoch: 8 | Time: 0m 15s\n",
      "\tTrain Loss: 1.877 | Train Acc: 42.78%\n",
      "\tVal.  Loss: 2.008 | Val Acc: 38.05%\n",
      "Epoch: 9 | Time: 0m 15s\n",
      "\tTrain Loss: 1.825 | Train Acc: 44.34%\n",
      "\tVal.  Loss: 1.984 | Val Acc: 39.10%\n",
      "Epoch: 10 | Time: 0m 16s\n",
      "\tTrain Loss: 1.777 | Train Acc: 46.21%\n",
      "\tVal.  Loss: 1.972 | Val Acc: 38.91%\n",
      "Epoch: 11 | Time: 0m 17s\n",
      "\tTrain Loss: 1.722 | Train Acc: 47.94%\n",
      "\tVal.  Loss: 1.957 | Val Acc: 39.31%\n",
      "Epoch: 12 | Time: 0m 15s\n",
      "\tTrain Loss: 1.687 | Train Acc: 48.58%\n",
      "\tVal.  Loss: 1.942 | Val Acc: 40.08%\n",
      "Epoch: 13 | Time: 0m 16s\n",
      "\tTrain Loss: 1.649 | Train Acc: 50.19%\n",
      "\tVal.  Loss: 1.936 | Val Acc: 39.72%\n",
      "Epoch: 14 | Time: 0m 15s\n",
      "\tTrain Loss: 1.608 | Train Acc: 51.09%\n",
      "\tVal.  Loss: 1.946 | Val Acc: 39.10%\n",
      "Epoch: 15 | Time: 0m 16s\n",
      "\tTrain Loss: 1.563 | Train Acc: 53.00%\n",
      "\tVal.  Loss: 1.918 | Val Acc: 40.62%\n",
      "Epoch: 16 | Time: 0m 15s\n",
      "\tTrain Loss: 1.535 | Train Acc: 53.41%\n",
      "\tVal.  Loss: 1.906 | Val Acc: 41.61%\n",
      "Epoch: 17 | Time: 0m 15s\n",
      "\tTrain Loss: 1.494 | Train Acc: 54.98%\n",
      "\tVal.  Loss: 1.907 | Val Acc: 41.33%\n",
      "Epoch: 18 | Time: 0m 16s\n",
      "\tTrain Loss: 1.466 | Train Acc: 55.53%\n",
      "\tVal.  Loss: 1.907 | Val Acc: 40.97%\n",
      "Epoch: 19 | Time: 0m 16s\n",
      "\tTrain Loss: 1.423 | Train Acc: 57.22%\n",
      "\tVal.  Loss: 1.901 | Val Acc: 41.23%\n",
      "Epoch: 20 | Time: 0m 15s\n",
      "\tTrain Loss: 1.387 | Train Acc: 58.01%\n",
      "\tVal.  Loss: 1.913 | Val Acc: 41.56%\n",
      "Epoch: 21 | Time: 0m 16s\n",
      "\tTrain Loss: 1.362 | Train Acc: 58.76%\n",
      "\tVal.  Loss: 1.905 | Val Acc: 41.30%\n",
      "Epoch: 22 | Time: 0m 15s\n",
      "\tTrain Loss: 1.329 | Train Acc: 60.27%\n",
      "\tVal.  Loss: 1.898 | Val Acc: 42.28%\n",
      "Epoch: 23 | Time: 0m 15s\n",
      "\tTrain Loss: 1.302 | Train Acc: 61.11%\n",
      "\tVal.  Loss: 1.901 | Val Acc: 42.30%\n",
      "Epoch: 24 | Time: 0m 17s\n",
      "\tTrain Loss: 1.273 | Train Acc: 61.78%\n",
      "\tVal.  Loss: 1.918 | Val Acc: 41.44%\n",
      "Epoch: 25 | Time: 0m 15s\n",
      "\tTrain Loss: 1.248 | Train Acc: 62.82%\n",
      "\tVal.  Loss: 1.908 | Val Acc: 42.42%\n",
      "Epoch: 26 | Time: 0m 14s\n",
      "\tTrain Loss: 1.209 | Train Acc: 63.55%\n",
      "\tVal.  Loss: 1.901 | Val Acc: 42.11%\n",
      "Epoch: 27 | Time: 0m 16s\n",
      "\tTrain Loss: 1.180 | Train Acc: 64.47%\n",
      "\tVal.  Loss: 1.916 | Val Acc: 42.01%\n",
      "Epoch: 28 | Time: 0m 15s\n",
      "\tTrain Loss: 1.157 | Train Acc: 65.59%\n",
      "\tVal.  Loss: 1.914 | Val Acc: 42.88%\n",
      "Epoch: 29 | Time: 0m 15s\n",
      "\tTrain Loss: 1.128 | Train Acc: 66.26%\n",
      "\tVal.  Loss: 1.919 | Val Acc: 41.39%\n",
      "Epoch: 30 | Time: 0m 15s\n",
      "\tTrain Loss: 1.111 | Train Acc: 67.03%\n",
      "\tVal.  Loss: 1.914 | Val Acc: 42.61%\n",
      "===== CNN with 30-epochs-100-n_filters =====\n",
      "Epoch: 1 | Time: 0m 22s\n",
      "\tTrain Loss: 2.795 | Train Acc: 12.30%\n",
      "\tVal.  Loss: 2.528 | Val Acc: 26.52%\n",
      "Epoch: 2 | Time: 0m 22s\n",
      "\tTrain Loss: 2.431 | Train Acc: 25.41%\n",
      "\tVal.  Loss: 2.296 | Val Acc: 31.73%\n",
      "Epoch: 3 | Time: 0m 22s\n",
      "\tTrain Loss: 2.220 | Train Acc: 31.96%\n",
      "\tVal.  Loss: 2.175 | Val Acc: 35.05%\n",
      "Epoch: 4 | Time: 0m 21s\n",
      "\tTrain Loss: 2.081 | Train Acc: 37.08%\n",
      "\tVal.  Loss: 2.106 | Val Acc: 35.43%\n",
      "Epoch: 5 | Time: 0m 21s\n",
      "\tTrain Loss: 1.992 | Train Acc: 39.44%\n",
      "\tVal.  Loss: 2.056 | Val Acc: 36.64%\n",
      "Epoch: 6 | Time: 0m 21s\n",
      "\tTrain Loss: 1.898 | Train Acc: 42.30%\n",
      "\tVal.  Loss: 2.015 | Val Acc: 38.93%\n",
      "Epoch: 7 | Time: 0m 22s\n",
      "\tTrain Loss: 1.831 | Train Acc: 44.28%\n",
      "\tVal.  Loss: 1.985 | Val Acc: 39.84%\n",
      "Epoch: 8 | Time: 0m 21s\n",
      "\tTrain Loss: 1.762 | Train Acc: 46.71%\n",
      "\tVal.  Loss: 1.958 | Val Acc: 39.76%\n",
      "Epoch: 9 | Time: 0m 22s\n",
      "\tTrain Loss: 1.694 | Train Acc: 49.20%\n",
      "\tVal.  Loss: 1.948 | Val Acc: 39.92%\n",
      "Epoch: 10 | Time: 0m 24s\n",
      "\tTrain Loss: 1.649 | Train Acc: 50.01%\n",
      "\tVal.  Loss: 1.926 | Val Acc: 41.01%\n",
      "Epoch: 11 | Time: 0m 22s\n",
      "\tTrain Loss: 1.589 | Train Acc: 51.81%\n",
      "\tVal.  Loss: 1.913 | Val Acc: 41.01%\n",
      "Epoch: 12 | Time: 0m 22s\n",
      "\tTrain Loss: 1.545 | Train Acc: 53.57%\n",
      "\tVal.  Loss: 1.906 | Val Acc: 41.76%\n",
      "Epoch: 13 | Time: 0m 21s\n",
      "\tTrain Loss: 1.485 | Train Acc: 55.50%\n",
      "\tVal.  Loss: 1.898 | Val Acc: 41.74%\n",
      "Epoch: 14 | Time: 0m 21s\n",
      "\tTrain Loss: 1.447 | Train Acc: 56.41%\n",
      "\tVal.  Loss: 1.890 | Val Acc: 42.32%\n",
      "Epoch: 15 | Time: 0m 21s\n",
      "\tTrain Loss: 1.395 | Train Acc: 58.04%\n",
      "\tVal.  Loss: 1.888 | Val Acc: 42.21%\n",
      "Epoch: 16 | Time: 0m 21s\n",
      "\tTrain Loss: 1.345 | Train Acc: 60.83%\n",
      "\tVal.  Loss: 1.886 | Val Acc: 42.08%\n",
      "Epoch: 17 | Time: 0m 22s\n",
      "\tTrain Loss: 1.299 | Train Acc: 61.07%\n",
      "\tVal.  Loss: 1.879 | Val Acc: 42.49%\n",
      "Epoch: 18 | Time: 0m 22s\n",
      "\tTrain Loss: 1.253 | Train Acc: 62.84%\n",
      "\tVal.  Loss: 1.900 | Val Acc: 41.94%\n",
      "Epoch: 19 | Time: 0m 22s\n",
      "\tTrain Loss: 1.215 | Train Acc: 64.21%\n",
      "\tVal.  Loss: 1.867 | Val Acc: 43.18%\n",
      "Epoch: 20 | Time: 0m 21s\n",
      "\tTrain Loss: 1.180 | Train Acc: 65.76%\n",
      "\tVal.  Loss: 1.894 | Val Acc: 42.72%\n",
      "Epoch: 21 | Time: 0m 22s\n",
      "\tTrain Loss: 1.137 | Train Acc: 66.77%\n",
      "\tVal.  Loss: 1.884 | Val Acc: 42.97%\n",
      "Epoch: 22 | Time: 0m 24s\n",
      "\tTrain Loss: 1.107 | Train Acc: 67.57%\n",
      "\tVal.  Loss: 1.883 | Val Acc: 44.01%\n",
      "Epoch: 23 | Time: 0m 21s\n",
      "\tTrain Loss: 1.071 | Train Acc: 68.76%\n",
      "\tVal.  Loss: 1.880 | Val Acc: 43.78%\n",
      "Epoch: 24 | Time: 0m 23s\n",
      "\tTrain Loss: 1.023 | Train Acc: 70.70%\n",
      "\tVal.  Loss: 1.899 | Val Acc: 42.59%\n",
      "Epoch: 25 | Time: 0m 21s\n",
      "\tTrain Loss: 0.996 | Train Acc: 71.33%\n",
      "\tVal.  Loss: 1.895 | Val Acc: 43.07%\n",
      "Epoch: 26 | Time: 0m 22s\n",
      "\tTrain Loss: 0.969 | Train Acc: 72.10%\n",
      "\tVal.  Loss: 1.906 | Val Acc: 43.28%\n",
      "Epoch: 27 | Time: 0m 21s\n",
      "\tTrain Loss: 0.927 | Train Acc: 73.57%\n",
      "\tVal.  Loss: 1.921 | Val Acc: 42.59%\n",
      "Epoch: 28 | Time: 0m 21s\n",
      "\tTrain Loss: 0.894 | Train Acc: 74.83%\n",
      "\tVal.  Loss: 1.922 | Val Acc: 43.21%\n",
      "Epoch: 29 | Time: 0m 22s\n",
      "\tTrain Loss: 0.867 | Train Acc: 75.06%\n",
      "\tVal.  Loss: 1.929 | Val Acc: 42.35%\n",
      "Epoch: 30 | Time: 0m 22s\n",
      "\tTrain Loss: 0.837 | Train Acc: 76.20%\n",
      "\tVal.  Loss: 1.917 | Val Acc: 43.47%\n",
      "===== CNN with 30-epochs-150-n_filters =====\n",
      "Epoch: 1 | Time: 0m 25s\n",
      "\tTrain Loss: 2.803 | Train Acc: 12.36%\n",
      "\tVal.  Loss: 2.510 | Val Acc: 23.84%\n",
      "Epoch: 2 | Time: 0m 23s\n",
      "\tTrain Loss: 2.400 | Train Acc: 26.43%\n",
      "\tVal.  Loss: 2.276 | Val Acc: 32.66%\n",
      "Epoch: 3 | Time: 0m 24s\n",
      "\tTrain Loss: 2.172 | Train Acc: 33.93%\n",
      "\tVal.  Loss: 2.160 | Val Acc: 34.61%\n",
      "Epoch: 4 | Time: 0m 25s\n",
      "\tTrain Loss: 2.020 | Train Acc: 38.73%\n",
      "\tVal.  Loss: 2.072 | Val Acc: 36.74%\n",
      "Epoch: 5 | Time: 0m 25s\n",
      "\tTrain Loss: 1.909 | Train Acc: 42.54%\n",
      "\tVal.  Loss: 2.034 | Val Acc: 39.15%\n",
      "Epoch: 6 | Time: 0m 26s\n",
      "\tTrain Loss: 1.815 | Train Acc: 45.14%\n",
      "\tVal.  Loss: 1.983 | Val Acc: 38.97%\n",
      "Epoch: 7 | Time: 0m 23s\n",
      "\tTrain Loss: 1.733 | Train Acc: 47.56%\n",
      "\tVal.  Loss: 1.954 | Val Acc: 39.61%\n",
      "Epoch: 8 | Time: 0m 24s\n",
      "\tTrain Loss: 1.661 | Train Acc: 49.97%\n",
      "\tVal.  Loss: 1.937 | Val Acc: 39.62%\n",
      "Epoch: 9 | Time: 0m 24s\n",
      "\tTrain Loss: 1.597 | Train Acc: 52.14%\n",
      "\tVal.  Loss: 1.920 | Val Acc: 40.36%\n",
      "Epoch: 10 | Time: 0m 25s\n",
      "\tTrain Loss: 1.534 | Train Acc: 53.46%\n",
      "\tVal.  Loss: 1.907 | Val Acc: 40.03%\n",
      "Epoch: 11 | Time: 0m 24s\n",
      "\tTrain Loss: 1.472 | Train Acc: 55.83%\n",
      "\tVal.  Loss: 1.892 | Val Acc: 41.61%\n",
      "Epoch: 12 | Time: 0m 26s\n",
      "\tTrain Loss: 1.416 | Train Acc: 57.72%\n",
      "\tVal.  Loss: 1.892 | Val Acc: 41.62%\n",
      "Epoch: 13 | Time: 0m 25s\n",
      "\tTrain Loss: 1.364 | Train Acc: 59.38%\n",
      "\tVal.  Loss: 1.880 | Val Acc: 41.16%\n",
      "Epoch: 14 | Time: 0m 26s\n",
      "\tTrain Loss: 1.309 | Train Acc: 61.23%\n",
      "\tVal.  Loss: 1.884 | Val Acc: 40.94%\n",
      "Epoch: 15 | Time: 0m 25s\n",
      "\tTrain Loss: 1.263 | Train Acc: 62.54%\n",
      "\tVal.  Loss: 1.886 | Val Acc: 41.85%\n",
      "Epoch: 16 | Time: 0m 25s\n",
      "\tTrain Loss: 1.217 | Train Acc: 64.00%\n",
      "\tVal.  Loss: 1.871 | Val Acc: 42.33%\n",
      "Epoch: 17 | Time: 0m 23s\n",
      "\tTrain Loss: 1.163 | Train Acc: 65.95%\n",
      "\tVal.  Loss: 1.873 | Val Acc: 41.63%\n",
      "Epoch: 18 | Time: 0m 24s\n",
      "\tTrain Loss: 1.119 | Train Acc: 67.39%\n",
      "\tVal.  Loss: 1.888 | Val Acc: 41.98%\n",
      "Epoch: 19 | Time: 0m 26s\n",
      "\tTrain Loss: 1.061 | Train Acc: 69.53%\n",
      "\tVal.  Loss: 1.883 | Val Acc: 42.23%\n",
      "Epoch: 20 | Time: 0m 25s\n",
      "\tTrain Loss: 1.024 | Train Acc: 70.13%\n",
      "\tVal.  Loss: 1.880 | Val Acc: 42.39%\n",
      "Epoch: 21 | Time: 0m 25s\n",
      "\tTrain Loss: 0.991 | Train Acc: 72.06%\n",
      "\tVal.  Loss: 1.889 | Val Acc: 42.07%\n",
      "Epoch: 22 | Time: 0m 24s\n",
      "\tTrain Loss: 0.954 | Train Acc: 72.68%\n",
      "\tVal.  Loss: 1.892 | Val Acc: 42.53%\n",
      "Epoch: 23 | Time: 0m 24s\n",
      "\tTrain Loss: 0.909 | Train Acc: 74.35%\n",
      "\tVal.  Loss: 1.893 | Val Acc: 43.29%\n",
      "Epoch: 24 | Time: 0m 24s\n",
      "\tTrain Loss: 0.867 | Train Acc: 75.69%\n",
      "\tVal.  Loss: 1.904 | Val Acc: 42.08%\n",
      "Epoch: 25 | Time: 0m 24s\n",
      "\tTrain Loss: 0.845 | Train Acc: 76.16%\n",
      "\tVal.  Loss: 1.926 | Val Acc: 41.75%\n",
      "Epoch: 26 | Time: 0m 24s\n",
      "\tTrain Loss: 0.801 | Train Acc: 77.29%\n",
      "\tVal.  Loss: 1.912 | Val Acc: 43.22%\n",
      "Epoch: 27 | Time: 0m 24s\n",
      "\tTrain Loss: 0.772 | Train Acc: 78.50%\n",
      "\tVal.  Loss: 1.919 | Val Acc: 43.56%\n",
      "Epoch: 28 | Time: 0m 24s\n",
      "\tTrain Loss: 0.742 | Train Acc: 79.48%\n",
      "\tVal.  Loss: 1.923 | Val Acc: 43.39%\n",
      "Epoch: 29 | Time: 0m 25s\n",
      "\tTrain Loss: 0.704 | Train Acc: 80.94%\n",
      "\tVal.  Loss: 1.931 | Val Acc: 43.10%\n",
      "Epoch: 30 | Time: 0m 23s\n",
      "\tTrain Loss: 0.690 | Train Acc: 81.07%\n",
      "\tVal.  Loss: 1.936 | Val Acc: 42.93%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = [10,20,30]\n",
    "n_filters = [50,100,150]\n",
    "\n",
    "\n",
    "for num_epoch in num_epochs:\n",
    "    for n_filter in n_filters:\n",
    "        \n",
    "        input_dim  = len(vocab)\n",
    "        emb_dim    = 300\n",
    "        output_dim = 17 #17 classes\n",
    "\n",
    "        #for cnn\n",
    "        dropout = 0.5\n",
    "        filter_sizes = [3, 4, 5]\n",
    "\n",
    "        params={\"model\":\"CNN\", \"num_epochs\":num_epoch, \"n_filters\":n_filter, \"filter_sizes\":filter_sizes, \"input_dim\":input_dim, \"emb_dim\":emb_dim, \"output_dim\":output_dim, \"dropout\":0.5}\n",
    "        mlflow.start_run(run_name=f\"CNN1000-{params['num_epochs']}-epochs-{params['n_filters']}-n_filters\")\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        print(\"=\"*5, f\"CNN with {params['num_epochs']}-epochs-{params['n_filters']}-n_filters\",\"=\"*5)\n",
    "\n",
    "        model = CNN(input_dim, emb_dim, output_dim, dropout, n_filter, filter_sizes).to(device)\n",
    "        model.apply(initialize_weights)\n",
    "        model.embedding.weight.data = fast_embedding\n",
    "\n",
    "        lr=0.05\n",
    "        #training hyperparameters\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs = [],[],[],[]\n",
    "        best_valid_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epoch):\n",
    "            start_time = time.time()\n",
    "                \n",
    "            train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
    "            valid_loss, valid_acc = evaluate(model, val_loader, criterion, val_loader_length)\n",
    "                \n",
    "            #for plotting\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_losses.append(valid_loss)\n",
    "            val_accs.append(valid_acc)\n",
    "                \n",
    "            end_time = time.time()\n",
    "                \n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            mlflow.log_metric(key=\"train_loss\", value=train_loss, step=epoch)\n",
    "            mlflow.log_metric(key=\"train_acc\", value=train_acc, step=epoch)\n",
    "            mlflow.log_metric(key=\"val_loss\", value=valid_loss, step=epoch)\n",
    "            mlflow.log_metric(key=\"val_acc\", value=valid_acc, step=epoch)\n",
    "                \n",
    "            \n",
    "            #early stopping\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                mlflow.pytorch.log_model(model, \"model\")\n",
    "                \n",
    "            print(f'Epoch: {epoch+1} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "            print(f'\\tVal.  Loss: {valid_loss:.3f} | Val Acc: {valid_acc*100:.2f}%')\n",
    "        mlflow.log_metric(key=\"min_val_loss\", value=min(val_losses), step=epoch)    \n",
    "        mlflow.end_run()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
